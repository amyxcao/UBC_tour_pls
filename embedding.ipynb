{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408783c0",
   "metadata": {},
   "source": [
    "# Using BEG-VL-Base model to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa9360f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"BAAI/BGE-VL-base\" # or \"BAAI/BGE-VL-large\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dense_model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(device) # You must set trust_remote_code=True\n",
    "dense_model_processor = AutoProcessor.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "dense_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9d73d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_text_embeddings(texts: list[str]) -> list:\n",
    "    if not texts:\n",
    "        return []\n",
    "    inputs = dense_model_processor(text=texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    return dense_model.get_text_features(**inputs).cpu()\n",
    "\n",
    "def get_image_embeddings(image_paths: list[str]) -> list:\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    images = [Image.open(image_path).convert(\"RGB\") for image_path in image_paths]\n",
    "    inputs = dense_model_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "    return dense_model.get_image_features(**inputs).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cb28d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello, world!\", \"This is a test sentence.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "541d768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = get_text_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3fc1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [\n",
    "#     r\"output\\Objectifying_China\\auto\\images\\0bf9d41010da900a0abb7048118e147c6e962eec73c6962affcf498cec014420.jpg\",\n",
    "#     r\"output\\Objectifying_China\\auto\\images\\0bf9d41010da900a0abb7048118e147c6e962eec73c6962affcf498cec014420.jpg\"\n",
    "# ]\n",
    "# image_embeddings = get_image_embeddings(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f229e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sparse_model_tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-v3\")\n",
    "sparse_model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-v3\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a06051c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_embeddings(texts: list[str]):\n",
    "    if not texts:\n",
    "        return []\n",
    "    tokens = sparse_model_tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = sparse_model(**tokens)\n",
    "    sparse_embedding = torch.max(torch.log(1 + torch.relu(outputs.logits)) * tokens.attention_mask.unsqueeze(-1), dim=1)[0].detach().cpu()\n",
    "    \n",
    "    # convert to pinecone sparse format\n",
    "    res = []\n",
    "    for i in range(len(sparse_embedding)):\n",
    "        indices = sparse_embedding[i].nonzero().squeeze().tolist()\n",
    "        values = sparse_embedding[i, indices].tolist()\n",
    "        res.append({\"indices\": indices, \"values\": values})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbd0c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = get_sparse_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e754cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0f122065",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def generate_embedding_file(input_file, output_dir=None):\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        contents = json.load(f)\n",
    "\n",
    "    sections = {}\n",
    "    for section_id, section in contents.items():\n",
    "        new_section = []\n",
    "        text_batch = []\n",
    "        image_batch = []\n",
    "        \n",
    "        for num, content in enumerate(section):  \n",
    "            # processing text batchs\n",
    "            if len(text_batch) >= batch_size or num == len(section) - 1:\n",
    "                texts = section['text']\n",
    "                dense_embeddings = get_text_embeddings(texts)\n",
    "                sparse_embeddings = get_sparse_embeddings(texts)\n",
    "                for idx, content in enumerate(text_batch):\n",
    "                    content['dense_embeddings'] = dense_embeddings[idx].tolist()\n",
    "                    # adjusting sparse vector format into dict\n",
    "                    content['sparse_embeddings'] = sparse_embeddings[idx]\n",
    "                new_section.extend(text_batch)\n",
    "                text_batch = []\n",
    "\n",
    "            # processing image batchs\n",
    "            if len(image_batch) >= batch_size or num == len(section) - 1:\n",
    "                image_paths = section['img_path']\n",
    "                dense_embeddings = get_image_embeddings(image_paths)\n",
    "                for idx, content in enumerate(image_batch):\n",
    "                    content['dense_embeddings'] = dense_embeddings[idx].tolist()\n",
    "                new_section.extend(image_batch)\n",
    "                image_batch = []\n",
    "\n",
    "            # next batch\n",
    "            text_batch.append(content)\n",
    "            if(len(section['img_path']) != 0):\n",
    "                image_batch.append(content)\n",
    "                \n",
    "            # # preparing for batches\n",
    "            # if content['type'] == 'text':\n",
    "            #     text_batch.append(content)\n",
    "            # elif content['type'] == 'image':\n",
    "            #     image_batch.append(content)\n",
    "\n",
    "        sections[section_id] = new_section\n",
    "\n",
    "    output_filename = \"embeddings.json\"\n",
    "    output_file = os.path.join(output_dir, output_filename)\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sections, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "094fc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embedding_file(\"output/Objectifying_China/tagged/en_contents_doc_chunked.json\", \n",
    "                        \"output/Objectifying_China/embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cd8b9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_file(input_file, output_dir=None):\n",
    "    import os, json\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(input_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        contents = json.load(f)\n",
    "\n",
    "    # Process each section (each section is a dictionary)\n",
    "    for section_id, section in contents.items():\n",
    "        # Process text field\n",
    "        raw_text = section.get(\"text\", \"\")\n",
    "        if raw_text:\n",
    "            # get_text_embeddings expects a list of strings\n",
    "            dense_txt = get_text_embeddings([raw_text])[0].tolist()\n",
    "            sparse_txt = get_sparse_embeddings([raw_text])[0]\n",
    "            section[\"dense_text_embedding\"] = dense_txt\n",
    "            section[\"sparse_text_embedding\"] = sparse_txt\n",
    "\n",
    "        # Process any images if present (assuming \"img_path\" is a list)\n",
    "        image_paths = section.get(\"img_path\", [])\n",
    "        if image_paths:\n",
    "            dense_image_embeddings = get_image_embeddings(image_paths)\n",
    "            # Convert each tensor to a list if needed\n",
    "            section[\"dense_image_embeddings\"] = [emb.tolist() for emb in dense_image_embeddings]\n",
    "\n",
    "    # Write back the results to a new file name\n",
    "    base = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    out_path = os.path.join(output_dir, f\"{base}_embeddings.json\")\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(contents, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eeafa06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fcxhje1w', {'id': 'fcxhje1w', 'type': 'section', 'header': 'Vase', 'text': '', 'page_idx': [], 'img_path': [], 'img_caption': [], 'img_footnote': [], 'time_period': [], 'materiality': [], 'region': [], 'colour': [], 'purpose': [], 'themes': [], 'exhibit': 'Objectifying China'})\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "\n",
    "path = \"output/Objectifying_China/embeddings/en_contents_doc_chunked_embeddings.json\"\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(list(data.items())[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276275d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
