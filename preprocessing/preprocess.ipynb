{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71c5a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from uuid import uuid4\n",
    "from cuid2 import Cuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d57941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_language(text):\n",
    "    \"\"\"\n",
    "    Classifies a string into 'en', 'zh', or 'mixed' based on its characters, ignoring punctuation.\n",
    "    - 'en': Contains only English letters (A-Z, a-z).\n",
    "    - 'zh': Contains only Chinese characters (including Cantonese-specific ones).\n",
    "    - 'mixed': Contains both, neither, or other characters (e.g., numbers, symbols).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"mixed\"\n",
    "    \n",
    "    # 定义英文和中文标点\n",
    "    english_punctuation = r'[.,!?;:\\'\\\"()\\[\\]{}\\-\\\\/@#$%^&*+=|~`]'\n",
    "    chinese_punctuation = r'[，。！？；：、“”‘’（）【】{}—…《》〈〉]'\n",
    "    \n",
    "    # 去除英文和中文标点\n",
    "    import re\n",
    "    cleaned_text = re.sub(english_punctuation, '', text)\n",
    "    cleaned_text = re.sub(chinese_punctuation, '', cleaned_text)\n",
    "    \n",
    "    has_english = False\n",
    "    has_chinese = False\n",
    "    \n",
    "    for char in cleaned_text:\n",
    "        # Check for English letters (A-Z, a-z)\n",
    "        if ('\\u0041' <= char <= '\\u005A') or ('\\u0061' <= char <= '\\u007A'):\n",
    "            has_english = True\n",
    "        # Check for Chinese characters (CJK Unified Ideographs and extensions)\n",
    "        elif ('\\u4E00' <= char <= '\\u9FFF') or ('\\u3400' <= char <= '\\u4DBF') or ('\\u20000' <= char <= '\\u2A6DF'):\n",
    "            has_chinese = True\n",
    "        # Other characters (e.g., numbers, spaces) don't affect en/zh but contribute to mixed\n",
    "        \n",
    "    if has_english and has_chinese:\n",
    "        return \"mixed\"\n",
    "    elif has_english:\n",
    "        return \"en\"\n",
    "    elif has_chinese:\n",
    "        return \"zh\"\n",
    "    else:\n",
    "        return \"mixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3847f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_languages(text):\n",
    "    \"\"\"\n",
    "    Separates Chinese and English text into two strings, preserving punctuation and numbers\n",
    "    in their original language context. If Chinese is dominant (>70% of alphabetic chars),\n",
    "    the entire text is assigned to chinese_text.\n",
    "    Args:\n",
    "        text (str): Input string containing mixed Chinese and English text.\n",
    "    Returns:\n",
    "        list: [chinese_text, english_text], where each includes respective language segments\n",
    "              with punctuation and numbers preserved.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return [\"\", \"\"]\n",
    "    \n",
    "    # Define Chinese Unicode ranges (CJK Unified Ideographs and Extensions)\n",
    "    def is_chinese(char):\n",
    "        return (0x4E00 <= ord(char) <= 0x9FFF or  # CJK Unified Ideographs\n",
    "                0x3400 <= ord(char) <= 0x4DBF or  # CJK Extension A\n",
    "                0x20000 <= ord(char) <= 0x2A6DF)  # CJK Extensions B-F\n",
    "    \n",
    "    # Count Chinese and English alphabetic characters\n",
    "    chinese_count = 0\n",
    "    english_count = 0\n",
    "    for char in text:\n",
    "        if is_chinese(char):\n",
    "            chinese_count += 1\n",
    "        elif char.isalpha() and not is_chinese(char):\n",
    "            english_count += 1\n",
    "    \n",
    "    # If Chinese dominates (>70% of alphabetic characters), return full text as Chinese\n",
    "    total_alphabetic = chinese_count + english_count\n",
    "    if total_alphabetic == 0 or (chinese_count / total_alphabetic) > 0.65:\n",
    "        return [text, \"\"]\n",
    "    \n",
    "    # Separate languages, keeping punctuation and numbers with the current language\n",
    "    chinese_chars = []\n",
    "    english_chars = []\n",
    "    current_is_chinese = None\n",
    "    \n",
    "    for char in text:\n",
    "        is_char_chinese = is_chinese(char)\n",
    "        \n",
    "        # Initialize or switch language context based on alphabetic characters\n",
    "        if char.isalpha():\n",
    "            if current_is_chinese is None or current_is_chinese != is_char_chinese:\n",
    "                current_is_chinese = is_char_chinese\n",
    "        \n",
    "        # Append to the current language context (including punctuation and numbers)\n",
    "        if current_is_chinese is None:\n",
    "            # If no alphabetic chars yet, default to English for non-alphabetic chars\n",
    "            english_chars.append(char)\n",
    "        elif current_is_chinese:\n",
    "            chinese_chars.append(char)\n",
    "        else:\n",
    "            english_chars.append(char)\n",
    "    \n",
    "    # Convert character lists to strings\n",
    "    chinese_text = \"\".join(chinese_chars)\n",
    "    english_text = \"\".join(english_chars)\n",
    "    \n",
    "    return [chinese_text, english_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febcfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tang dynasty 618–907   \\nFive Dynasties and Ten Kingdoms 907–979   \\nLiao dynasty 916–1125   \\nSong dynasty 960–1279   \\n- Northern Song 960–1127   \\n- Southern Song 1127–1279']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to D:\\AppData\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def split_paragraph(paragraph):\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    return sentences\n",
    "\n",
    "# paragraph = \"This is the first sentence. \\n\\n This is the second sentence! \\nIs this the third sentence?\"\n",
    "paragraph = \"Tang dynasty 618–907   \\nFive Dynasties and Ten Kingdoms 907–979   \\nLiao dynasty 916–1125   \\nSong dynasty 960–1279   \\n- Northern Song 960–1127   \\n- Southern Song 1127–1279\"\n",
    "sentences = split_paragraph(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9961f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipe(content_list_path, output_dir):\n",
    "    # === LOG BASIC INFOMATION ===\n",
    "    print(f\"=== Starting preprocessing pipeline ===\")\n",
    "    print(f\"Input file: {content_list_path}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "\n",
    "    # === CREATE CUID GENERATOR ===\n",
    "    print(f\"=== Creating CUID Generator ===\")\n",
    "    cuid = Cuid(length=8)\n",
    "    \n",
    "\n",
    "    # === LOAD CONTENT LIST ===\n",
    "    contents = json.load(open(content_list_path, 'r', encoding='utf-8'))\n",
    "    print(f\"Loaded {len(contents)} total content items\")\n",
    "    \n",
    "\n",
    "    # === GET TEXT AND IMAGE CONTENTS ===\n",
    "    text_contents = [content for content in contents if content['type'] == 'text']\n",
    "    image_contents = [content for content in contents if content['type'] == 'image']\n",
    "    print(f\"Content breakdown: {len(text_contents)} text items, {len(image_contents)} image items\")\n",
    "\n",
    "\n",
    "    # === GET EN AND ZH CONTENTS ===\n",
    "    print(\"\\n--- Language Classification ---\")\n",
    "    en_contents = [content for content in text_contents if classify_language(content['text']) == 'en']\n",
    "    zh_contents = [content for content in text_contents if classify_language(content['text']) == 'zh']\n",
    "    mixed_contents = [content for content in text_contents if classify_language(content['text']) == 'mixed']\n",
    "    print(f\"Classification results:\")\n",
    "    print(f\"  • English-only content: {len(en_contents)} items\")\n",
    "    print(f\"  • Chinese-only content: {len(zh_contents)} items\")  \n",
    "    print(f\"  • Mixed-language content: {len(mixed_contents)} items\")\n",
    "\n",
    "    # === PROCESS MIXED LANGUAGE CONTENT ===\n",
    "    print(\"\\n--- Processing Mixed Language Content ---\")\n",
    "    chinese_separated_contents = []\n",
    "    english_separated_contents = []\n",
    "    \n",
    "    if mixed_contents:\n",
    "        print(f\"Processing {len(mixed_contents)} mixed-language items...\")\n",
    "        for i, content in enumerate(mixed_contents):\n",
    "            chinese_text, english_text = separate_languages(content['text'])\n",
    "            if chinese_text.strip():\n",
    "                chinese_separated_contents.append({\n",
    "                    **content,\n",
    "                    'text': chinese_text,\n",
    "                })\n",
    "            \n",
    "            if english_text.strip():\n",
    "                english_separated_contents.append({\n",
    "                    **content,\n",
    "                    'text': english_text,\n",
    "                })\n",
    "            # Progress indicator for large datasets\n",
    "            if (i + 1) % 100 == 0 or (i + 1) == len(mixed_contents):\n",
    "                print(f\"  Progress: {i + 1}/{len(mixed_contents)} items processed\")\n",
    "        print(f\"Separation results:\")\n",
    "        print(f\"  • Chinese portions extracted: {len(chinese_separated_contents)} items\")\n",
    "        print(f\"  • English portions extracted: {len(english_separated_contents)} items\")\n",
    "    else:\n",
    "        print(\"No mixed-language content to process\")\n",
    "\n",
    "    # Add separated contents back to the original lists\n",
    "    print(\"\\n--- Merging Separated Content ---\")\n",
    "    original_en_count = len(en_contents)\n",
    "    original_zh_count = len(zh_contents)\n",
    "    en_contents.extend(english_separated_contents)\n",
    "    zh_contents.extend(chinese_separated_contents)\n",
    "    \n",
    "    print(f\"English content: {original_en_count} → {len(en_contents)} (+{len(english_separated_contents)})\")\n",
    "    print(f\"Chinese content: {original_zh_count} → {len(zh_contents)} (+{len(chinese_separated_contents)})\")\n",
    "\n",
    "\n",
    "    # === ADD IMAGE CONTENTS ===\n",
    "    print(\"\\n--- Adding Image Content ---\")\n",
    "    # convert image path to absolute path if necessary\n",
    "    for content in image_contents:\n",
    "        content['img_path'] = os.path.join(os.path.abspath(output_dir), \"../auto/\", content['img_path'])\n",
    "    en_contents.extend(image_contents)\n",
    "    zh_contents.extend(image_contents)\n",
    "    \n",
    "    print(f\"After adding {len(image_contents)} images to both languages:\")\n",
    "    print(f\"  • Total English content: {len(en_contents)} items\")\n",
    "    print(f\"  • Total Chinese content: {len(zh_contents)} items\")\n",
    "\n",
    "\n",
    "    # === SORT CONTENTS BY PAGE INDEX ===\n",
    "    def sort_contents_by_page(contents):\n",
    "        return sorted(contents, key=lambda x: x.get('page_idx', 0))\n",
    "\n",
    "    print(\"\\n--- Sorting by Page Index ---\")\n",
    "    sorted_en_contents = sort_contents_by_page(en_contents)\n",
    "    sorted_zh_contents = sort_contents_by_page(zh_contents)\n",
    "    print(\"Content sorted by page index\")\n",
    "\n",
    "\n",
    "    # === CLEAN EMPTY CONTENTS ===\n",
    "    def clear_empty_contents(contents, lang_name):\n",
    "        original_count = len(contents)\n",
    "        cleared_contents = [content for content in contents if content.get('text') or content.get('img_path')]\n",
    "        removed_count = original_count - len(cleared_contents)\n",
    "        print(f\"  {lang_name}: Removed {removed_count} empty items, kept {len(cleared_contents)} items\")\n",
    "        return cleared_contents\n",
    "\n",
    "    print(\"\\n--- Cleaning Empty Content ---\")\n",
    "    clean_en_contents = clear_empty_contents(sorted_en_contents, \"English\")\n",
    "    clean_zh_contents = clear_empty_contents(sorted_zh_contents, \"Chinese\")\n",
    "\n",
    "\n",
    "    # === CHUNKING CONTENTS ===\n",
    "    def chunk_contents(contents):\n",
    "        new_contents = []\n",
    "        for content in contents:\n",
    "            if content['type'] == 'image':\n",
    "                new_contents.append({\n",
    "                    **content,\n",
    "                    'id': str(cuid.generate()),\n",
    "                })\n",
    "\n",
    "            elif content['type'] == 'text':\n",
    "                # Split text into chunks\n",
    "                chunks = split_paragraph(content['text'])\n",
    "                for chunk in chunks:\n",
    "                    new_contents.append({\n",
    "                        **content,\n",
    "                        'text': chunk,\n",
    "                        'id': str(cuid.generate()),\n",
    "                    })\n",
    "        \n",
    "        return new_contents\n",
    "    \n",
    "    print(\"\\n--- Chunking Text Content ---\")\n",
    "    chunked_en_contents = chunk_contents(clean_en_contents)\n",
    "    chunked_zh_contents = chunk_contents(clean_zh_contents)\n",
    "    print(f\"  English content chunked into {len(chunked_en_contents)} items\")\n",
    "    print(f\"  Chinese content chunked into {len(chunked_zh_contents)} items\")\n",
    "\n",
    "    \n",
    "    # === GROUPING CONTENTS INTO SECTIONS ===\n",
    "    def group_contents(contents, lang_name):\n",
    "        print(f\"  Grouping {lang_name} content by topics...\")\n",
    "        final_contents = {}\n",
    "        grouped_contents = []\n",
    "        topic_count = 0\n",
    "        \n",
    "        for content in contents:\n",
    "            if 'text_level' in content and grouped_contents:\n",
    "                section_id = str(cuid.generate())\n",
    "                final_contents[section_id] = grouped_contents\n",
    "                topic_count += 1\n",
    "                grouped_contents = []\n",
    "            \n",
    "            grouped_contents.append(content)\n",
    "        \n",
    "        if grouped_contents:\n",
    "            section_id = str(cuid.generate())\n",
    "            final_contents[section_id] = grouped_contents\n",
    "            topic_count += 1\n",
    "        \n",
    "        print(f\"  {lang_name} grouped into {topic_count} topic sections\")\n",
    "        \n",
    "        # Print statistics for each group\n",
    "        for i, group in list(final_contents.items())[:5]:  # Show first 5 groups to avoid spam\n",
    "            text_count = len([c for c in group if c.get('type') == 'text'])\n",
    "            img_count = len([c for c in group if c.get('type') == 'image'])\n",
    "            print(f\"    Topic {i}: {text_count} text + {img_count} images\")\n",
    "        \n",
    "        if len(final_contents) > 5:\n",
    "            print(f\"    ... and {len(final_contents) - 5} more topics\")\n",
    "        \n",
    "        return final_contents\n",
    "\n",
    "    print(\"\\n--- Grouping by Topics ---\")\n",
    "    grouped_en_contents = group_contents(chunked_en_contents, \"English\")\n",
    "    grouped_zh_contents = group_contents(chunked_zh_contents, \"Chinese\")\n",
    "\n",
    "\n",
    "    # === SAVE RESULTS ===\n",
    "    print(\"\\n--- Saving Results ---\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    en_output_path = os.path.join(output_dir, 'en_contents.json')\n",
    "    zh_output_path = os.path.join(output_dir, 'zh_contents.json')\n",
    "    \n",
    "    with open(en_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(grouped_en_contents, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"English content saved to: {en_output_path}\")\n",
    "    \n",
    "    with open(zh_output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(grouped_zh_contents, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Chinese content saved to: {zh_output_path}\")\n",
    "    \n",
    "    print(f\"\\n=== Preprocessing completed successfully ===\")\n",
    "    print(f\"Final results:\")\n",
    "    print(f\"  • English: {len(grouped_en_contents)} topic groups\")\n",
    "    print(f\"  • Chinese: {len(grouped_zh_contents)} topic groups\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f15d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting preprocessing pipeline ===\n",
      "Input file: output\\Objectifying_China\\auto\\Objectifying_China_content_list.json\n",
      "Output directory: output\\Objectifying_China\\preprocessed\n",
      "=== Creating CUID Generator ===\n",
      "Loaded 820 total content items\n",
      "Content breakdown: 745 text items, 74 image items\n",
      "\n",
      "--- Language Classification ---\n",
      "Classification results:\n",
      "  • English-only content: 286 items\n",
      "  • Chinese-only content: 228 items\n",
      "  • Mixed-language content: 231 items\n",
      "\n",
      "--- Processing Mixed Language Content ---\n",
      "Processing 231 mixed-language items...\n",
      "  Progress: 100/231 items processed\n",
      "  Progress: 200/231 items processed\n",
      "  Progress: 231/231 items processed\n",
      "Separation results:\n",
      "  • Chinese portions extracted: 131 items\n",
      "  • English portions extracted: 103 items\n",
      "\n",
      "--- Merging Separated Content ---\n",
      "English content: 286 → 389 (+103)\n",
      "Chinese content: 228 → 359 (+131)\n",
      "\n",
      "--- Adding Image Content ---\n",
      "After adding 74 images to both languages:\n",
      "  • Total English content: 463 items\n",
      "  • Total Chinese content: 433 items\n",
      "\n",
      "--- Sorting by Page Index ---\n",
      "Content sorted by page index\n",
      "\n",
      "--- Cleaning Empty Content ---\n",
      "  English: Removed 0 empty items, kept 463 items\n",
      "  Chinese: Removed 0 empty items, kept 433 items\n",
      "\n",
      "--- Chunking Text Content ---\n",
      "  English content chunked into 1027 items\n",
      "  Chinese content chunked into 448 items\n",
      "\n",
      "--- Grouping by Topics ---\n",
      "  Grouping English content by topics...\n",
      "  English grouped into 83 topic sections\n",
      "    Topic rukvggxl: 3 text + 2 images\n",
      "    Topic cofk23s8: 2 text + 0 images\n",
      "    Topic jt8hpesy: 5 text + 0 images\n",
      "    Topic edl82c0t: 2 text + 0 images\n",
      "    Topic arrvlvzz: 4 text + 1 images\n",
      "    ... and 78 more topics\n",
      "  Grouping Chinese content by topics...\n",
      "  Chinese grouped into 83 topic sections\n",
      "    Topic i7esbzni: 2 text + 2 images\n",
      "    Topic lyzgipaw: 2 text + 0 images\n",
      "    Topic yenyssbe: 5 text + 0 images\n",
      "    Topic tept606y: 2 text + 0 images\n",
      "    Topic zktzw290: 4 text + 1 images\n",
      "    ... and 78 more topics\n",
      "\n",
      "--- Saving Results ---\n",
      "English content saved to: output\\Objectifying_China\\preprocessed\\en_contents.json\n",
      "Chinese content saved to: output\\Objectifying_China\\preprocessed\\zh_contents.json\n",
      "\n",
      "=== Preprocessing completed successfully ===\n",
      "Final results:\n",
      "  • English: 83 topic groups\n",
      "  • Chinese: 83 topic groups\n",
      "--------------------------------------------------\n",
      "=== Starting preprocessing pipeline ===\n",
      "Input file: output\\Pictorial_Silk\\auto\\Pictorial_Silk_content_list.json\n",
      "Output directory: output\\Pictorial_Silk\\preprocessed\n",
      "=== Creating CUID Generator ===\n",
      "Loaded 545 total content items\n",
      "Content breakdown: 501 text items, 44 image items\n",
      "\n",
      "--- Language Classification ---\n",
      "Classification results:\n",
      "  • English-only content: 155 items\n",
      "  • Chinese-only content: 175 items\n",
      "  • Mixed-language content: 171 items\n",
      "\n",
      "--- Processing Mixed Language Content ---\n",
      "Processing 171 mixed-language items...\n",
      "  Progress: 100/171 items processed\n",
      "  Progress: 171/171 items processed\n",
      "Separation results:\n",
      "  • Chinese portions extracted: 78 items\n",
      "  • English portions extracted: 119 items\n",
      "\n",
      "--- Merging Separated Content ---\n",
      "English content: 155 → 274 (+119)\n",
      "Chinese content: 175 → 253 (+78)\n",
      "\n",
      "--- Adding Image Content ---\n",
      "After adding 44 images to both languages:\n",
      "  • Total English content: 318 items\n",
      "  • Total Chinese content: 297 items\n",
      "\n",
      "--- Sorting by Page Index ---\n",
      "Content sorted by page index\n",
      "\n",
      "--- Cleaning Empty Content ---\n",
      "  English: Removed 0 empty items, kept 318 items\n",
      "  Chinese: Removed 0 empty items, kept 297 items\n",
      "\n",
      "--- Chunking Text Content ---\n",
      "  English content chunked into 906 items\n",
      "  Chinese content chunked into 319 items\n",
      "\n",
      "--- Grouping by Topics ---\n",
      "  Grouping English content by topics...\n",
      "  English grouped into 52 topic sections\n",
      "    Topic kc3sbqzn: 7 text + 0 images\n",
      "    Topic q9xnyopo: 21 text + 2 images\n",
      "    Topic av3hhkz5: 4 text + 0 images\n",
      "    Topic ezp9o09x: 17 text + 0 images\n",
      "    Topic vhp4ad74: 68 text + 2 images\n",
      "    ... and 47 more topics\n",
      "  Grouping Chinese content by topics...\n",
      "  Chinese grouped into 67 topic sections\n",
      "    Topic z9dy8qcz: 5 text + 0 images\n",
      "    Topic coeylaqj: 19 text + 2 images\n",
      "    Topic prgsln7i: 12 text + 2 images\n",
      "    Topic vn9pyj2x: 9 text + 1 images\n",
      "    Topic d0cctsyt: 6 text + 1 images\n",
      "    ... and 62 more topics\n",
      "\n",
      "--- Saving Results ---\n",
      "English content saved to: output\\Pictorial_Silk\\preprocessed\\en_contents.json\n",
      "Chinese content saved to: output\\Pictorial_Silk\\preprocessed\\zh_contents.json\n",
      "\n",
      "=== Preprocessing completed successfully ===\n",
      "Final results:\n",
      "  • English: 52 topic groups\n",
      "  • Chinese: 67 topic groups\n",
      "--------------------------------------------------\n",
      "=== Starting preprocessing pipeline ===\n",
      "Input file: output\\Tradition to Contemporary\\auto\\Tradition to Contemporary_content_list.json\n",
      "Output directory: output\\Tradition to Contemporary\\preprocessed\n",
      "=== Creating CUID Generator ===\n",
      "Loaded 1206 total content items\n",
      "Content breakdown: 1075 text items, 130 image items\n",
      "\n",
      "--- Language Classification ---\n",
      "Classification results:\n",
      "  • English-only content: 302 items\n",
      "  • Chinese-only content: 390 items\n",
      "  • Mixed-language content: 383 items\n",
      "\n",
      "--- Processing Mixed Language Content ---\n",
      "Processing 383 mixed-language items...\n",
      "  Progress: 100/383 items processed\n",
      "  Progress: 200/383 items processed\n",
      "  Progress: 300/383 items processed\n",
      "  Progress: 383/383 items processed\n",
      "Separation results:\n",
      "  • Chinese portions extracted: 156 items\n",
      "  • English portions extracted: 298 items\n",
      "\n",
      "--- Merging Separated Content ---\n",
      "English content: 302 → 600 (+298)\n",
      "Chinese content: 390 → 546 (+156)\n",
      "\n",
      "--- Adding Image Content ---\n",
      "After adding 130 images to both languages:\n",
      "  • Total English content: 730 items\n",
      "  • Total Chinese content: 676 items\n",
      "\n",
      "--- Sorting by Page Index ---\n",
      "Content sorted by page index\n",
      "\n",
      "--- Cleaning Empty Content ---\n",
      "  English: Removed 0 empty items, kept 730 items\n",
      "  Chinese: Removed 0 empty items, kept 676 items\n",
      "\n",
      "--- Chunking Text Content ---\n",
      "  English content chunked into 1688 items\n",
      "  Chinese content chunked into 678 items\n",
      "\n",
      "--- Grouping by Topics ---\n",
      "  Grouping English content by topics...\n",
      "  English grouped into 153 topic sections\n",
      "    Topic cuibd0d9: 3 text + 2 images\n",
      "    Topic yqugbjn6: 2 text + 0 images\n",
      "    Topic uynqyano: 2 text + 0 images\n",
      "    Topic rkqr866l: 3 text + 0 images\n",
      "    Topic rjskbsme: 1 text + 0 images\n",
      "    ... and 148 more topics\n",
      "  Grouping Chinese content by topics...\n",
      "  Chinese grouped into 114 topic sections\n",
      "    Topic vnxmrw4s: 3 text + 2 images\n",
      "    Topic avxybp9a: 2 text + 0 images\n",
      "    Topic vb407i2c: 2 text + 0 images\n",
      "    Topic ml9cpx83: 3 text + 0 images\n",
      "    Topic g9i8xyh0: 1 text + 0 images\n",
      "    ... and 109 more topics\n",
      "\n",
      "--- Saving Results ---\n",
      "English content saved to: output\\Tradition to Contemporary\\preprocessed\\en_contents.json\n",
      "Chinese content saved to: output\\Tradition to Contemporary\\preprocessed\\zh_contents.json\n",
      "\n",
      "=== Preprocessing completed successfully ===\n",
      "Final results:\n",
      "  • English: 153 topic groups\n",
      "  • Chinese: 114 topic groups\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "preprocess_pipe(r'output\\Objectifying_China\\auto\\Objectifying_China_content_list.json', r'output\\Objectifying_China\\preprocessed')\n",
    "preprocess_pipe(r'output\\Pictorial_Silk\\auto\\Pictorial_Silk_content_list.json', r'output\\Pictorial_Silk\\preprocessed')\n",
    "preprocess_pipe(r'output\\Tradition to Contemporary\\auto\\Tradition to Contemporary_content_list.json', r'output\\Tradition to Contemporary\\preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8e4c271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from uuid import uuid4\n",
    "\n",
    "def propositionize(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        contents = json.load(f)\n",
    "\n",
    "    res = {}\n",
    "    for section_id, section_contents in contents.items():\n",
    "        res[section_id] = []\n",
    "        for content in section_contents:\n",
    "            if content['type'] == \"image\":\n",
    "                res[section_id].append(content)\n",
    "            elif content['type'] == \"text\":\n",
    "                if \"chunks\" not in content or content[\"chunks\"] is None:\n",
    "                    content.pop(\"chunks\", None)\n",
    "                    res[section_id].append(content)\n",
    "                    continue\n",
    "            \n",
    "                for chunk in content.get(\"chunks\", []):\n",
    "                    if chunk.strip():\n",
    "                        res[section_id].append({\"type\": content[\"type\"], \"page_idx\": content[\"page_idx\"], \"text\": chunk, \"id\": str(uuid4())[:8]})\n",
    "\n",
    "    with open(os.path.dirname(file_path) + \"/propositionized.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4b72f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "propositionize(r\"output\\Objectifying_China\\en_objectifying_contents_propositionized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12b6957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "propositionize(r\"output\\Pictorial_Silk\\en_contents_pictorial_silk_propositionized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "propositionize(r\"output\\Tradition to Contemporary\\en_contents_tradition_contemporary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f240bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_third_party(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        contents = json.load(f)\n",
    "\n",
    "    contents = contents[\"Tradition_Contemporary_removed\"]\n",
    "    res = {}\n",
    "    groups = []\n",
    "    for i in range(2, 94):\n",
    "        group = []\n",
    "        for (idx, content) in enumerate(contents):\n",
    "            # print(i, content[\"metadata\"][\"page\"])\n",
    "            if content[\"metadata\"][\"page\"] == i:\n",
    "                group.append(content)\n",
    "        \n",
    "        groups.append(group)\n",
    "\n",
    "    for idx, group in enumerate(groups):\n",
    "        section_id = str(uuid4())[:8]\n",
    "        res[section_id] = []\n",
    "        for content in group:\n",
    "            if content[\"metadata\"][\"type\"] == \"image\":\n",
    "                res[section_id].append({\n",
    "                    \"type\": \"image\",\n",
    "                    \"img_path\": content[\"image_path\"],\n",
    "                    \"id\": str(uuid4())[:8],\n",
    "                    \"page_idx\": content[\"metadata\"][\"page\"],\n",
    "                    \"img_caption\": content[\"image_caption\"]\n",
    "                })\n",
    "            elif content[\"metadata\"][\"type\"] == \"text\" or content[\"metadata\"][\"type\"] == \"header\":\n",
    "                text = content[\"text\"]\n",
    "                if text.strip():\n",
    "                    res[section_id].append({\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": text,\n",
    "                        \"id\": str(uuid4())[:8],\n",
    "                        \"page_idx\": content[\"metadata\"][\"page\"],\n",
    "                        \"chunks\": content.get(\"chunks\", None)\n",
    "                    })\n",
    "\n",
    "    with open(os.path.dirname(file_path) + \"/en_contents_tradition_contemporary.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, ensure_ascii=False, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbf30071",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_third_party(r\"output\\Tradition to Contemporary\\Tradition_Contemporary_removed_propositionized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc1e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
