{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6048db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# This is for the Traditional Contempoary PDF\n",
    "\n",
    "class Propositionizer1():\n",
    "    def __init__(self):\n",
    "        self.AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT_2\")\n",
    "        self.AZURE_KEY = os.getenv(\"AZURE_OPENAI_KEY_2\")\n",
    "        self.AZURE_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_2\")\n",
    "        self.AZURE_API_VERSION = os.getenv(\"AZURE_OPENAI_VERSION_2\")\n",
    "\n",
    "        self.client = AzureOpenAI(\n",
    "            azure_endpoint=self.AZURE_ENDPOINT,\n",
    "            api_key=self.AZURE_KEY,\n",
    "            api_version=self.AZURE_API_VERSION\n",
    "        )\n",
    "\n",
    "    def _build_prompt(self, content: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are an assistant tasked with converting meaningful sentences into concise, decontextualized propositions in JSON format.\n",
    "\n",
    "Before generating propositions, follow these steps:\n",
    "\n",
    "1. **Ignore Irrelevant Inputs**: Do NOT return propositions for texts that:\n",
    "   - Are shorter than a full sentence.\n",
    "   - Are just numbers, section titles, or isolated words (e.g., “Edition”, “15”, “All rights reserved”).\n",
    "   - Contain no meaningful content or context.\n",
    "\n",
    "2. **Simplify and Decompose**: For valid inputs, split compound sentences into multiple short, simple statements.\n",
    "   - Each proposition should stand alone without needing context.\n",
    "   - Replace pronouns like \"it\", \"they\", or \"this\" with explicit references.\n",
    "\n",
    "3. **Be Concise and Clear**: Rephrase only when needed. Avoid repeating information. Use neutral tone.\n",
    "\n",
    "4. **Format**: Return a list of plain strings in a JSON array.\n",
    "\n",
    "### Now apply this to the following input:\n",
    "\\\"\\\"\\\"{content}\\\"\\\"\\\"\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "    def _find_text(self, obj):\n",
    "        \"\"\"Recursively find 'text' in a dictionary\"\"\"\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if k == \"text\" and isinstance(v, str):\n",
    "                    return v.strip()\n",
    "                found = self._find_text(v)\n",
    "                if found:\n",
    "                    return found\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                found = self._find_text(item)\n",
    "                if found:\n",
    "                    return found\n",
    "        return None\n",
    "\n",
    "    def _looks_like_sentence(self, text: str) -> bool:\n",
    "        \"\"\"Heuristic: true if text starts with capital and ends with punctuation\"\"\"\n",
    "        return bool(re.match(r\"^[A-Z\\\"“‘].+?[\\.!?]$\", text.strip()))\n",
    "\n",
    "    def _safe_parse_json(self, text):\n",
    "        \"\"\"Extract and parse JSON array from LLM output\"\"\"\n",
    "        try:\n",
    "            if text.startswith(\"```json\") or text.startswith(\"```\"):\n",
    "                text = text.strip(\"`\").strip(\"json\").strip()\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def generate_propositions1(self, input_file: str, output_file: str):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        updated_data = {}\n",
    "        for section_key, entries in data.items():\n",
    "            print(f\"\\nProcessing section: {section_key}\")\n",
    "            for entry in tqdm(entries, desc=f\"Section {section_key}\"):\n",
    "                text = self._find_text(entry)\n",
    "                entry[\"chunks\"] = None  # Add the key always\n",
    "\n",
    "                if not text or not self._looks_like_sentence(text):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.AZURE_DEPLOYMENT,\n",
    "                        messages=[{\"role\": \"user\", \"content\": self._build_prompt(text)}],\n",
    "                        temperature=1,\n",
    "                    )\n",
    "                    proposition_text = response.choices[0].message.content.strip()\n",
    "                    parsed = self._safe_parse_json(proposition_text)\n",
    "\n",
    "                    if parsed and isinstance(parsed, list):\n",
    "                        entry[\"chunks\"] = parsed\n",
    "                    else:\n",
    "                        print(f\"Could not parse: {proposition_text}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {text[:40]}... — {e}\")\n",
    "                    entry[\"chunks\"] = None\n",
    "\n",
    "            updated_data[section_key] = entries\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(updated_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\nSaved updated JSON with cleaned propositions to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# This is for the silk and objectifying china PDF\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class Propositionizer2():\n",
    "    def __init__(self):\n",
    "        self.AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT_2\")\n",
    "        self.AZURE_KEY = os.getenv(\"AZURE_OPENAI_KEY_2\")\n",
    "        self.AZURE_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_2\")\n",
    "        self.AZURE_API_VERSION = os.getenv(\"AZURE_OPENAI_VERSION_2\")\n",
    "\n",
    "        self.client = AzureOpenAI(\n",
    "            azure_endpoint=self.AZURE_ENDPOINT,\n",
    "            api_key=self.AZURE_KEY,\n",
    "            api_version=self.AZURE_API_VERSION\n",
    "        )\n",
    "\n",
    "    def _build_prompt(self, content: str) -> str:\n",
    "        return f\"\"\"\n",
    "You are an assistant tasked with converting meaningful sentences into concise, decontextualized propositions in JSON format.\n",
    "\n",
    "Follow these rules:\n",
    "\n",
    "1. Ignore irrelevant inputs:\n",
    "   - Do NOT return propositions for texts that are just numbers, symbols, or isolated words (e.g., “Edition”, “15”, “All rights reserved”).\n",
    "   - Skip text that is a heading or label, rather than a full sentence.\n",
    "\n",
    "2. Process only meaningful full sentences:\n",
    "   - If the input is a complete sentence, return it as a one-item JSON array.\n",
    "   - If the input has multiple clauses, split them into multiple short, standalone propositions.\n",
    "\n",
    "3. Decompose and clarify:\n",
    "   - Split compound sentences.\n",
    "   - Replace vague pronouns like \"it\", \"they\", \"this\" with explicit references.\n",
    "   - Preserve original phrasing and facts when possible.\n",
    "\n",
    "4. Output format:\n",
    "   - Always return a JSON array of plain strings.\n",
    "   - Do not include markdown or explanations—only return the list.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Input: \"The distinctive floral spray and pomegranate designs on this plate were probably developed during the Xuande period.\"\n",
    "Output: [\"The floral spray and pomegranate designs on this plate were probably developed during the Xuande period.\"]\n",
    "\n",
    "Input: \"Curator Kikki Lam\"\n",
    "Output: []  (do not generate output for non-sentences)\n",
    "\n",
    "Now apply these rules to the following input:\n",
    "\n",
    "\\\"\\\"\\\"{content}\\\"\\\"\\\"\n",
    "\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "    def _find_text(self, obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if k == \"text\" and isinstance(v, str):\n",
    "                    return v.strip()\n",
    "                found = self._find_text(v)\n",
    "                if found:\n",
    "                    return found\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                found = self._find_text(item)\n",
    "                if found:\n",
    "                    return found\n",
    "        return None\n",
    "\n",
    "    def _safe_parse_json(self, text):\n",
    "        try:\n",
    "            parsed = json.loads(text)\n",
    "            if isinstance(parsed, str):\n",
    "                return [parsed]\n",
    "            return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            # Clean markdown/codeblock formatting\n",
    "            text = text.strip().strip(\"`\").strip(\"json\").strip()\n",
    "            try:\n",
    "                parsed = json.loads(text)\n",
    "                if isinstance(parsed, str):\n",
    "                    return [parsed]\n",
    "                return parsed\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def _looks_like_full_sentence(self, text):\n",
    "        \"\"\"Heuristic: full sentence starts with capital, ends with punctuation.\"\"\"\n",
    "        return bool(re.match(r\"^[A-Z\\\"“‘].+?[\\.!?]$\", text.strip()))\n",
    "\n",
    "    def generate_propositions2(self, input_file: str, output_file: str):\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        updated_data = {}\n",
    "\n",
    "        for section_key, entries in data.items():\n",
    "            valid_entries = []\n",
    "            for entry in entries:\n",
    "                text = self._find_text(entry)\n",
    "                if text and self._looks_like_full_sentence(text):\n",
    "                    valid_entries.append((entry, text))\n",
    "\n",
    "            subset_size = len(valid_entries)\n",
    "            subset = valid_entries[:subset_size]\n",
    "\n",
    "            for entry, text in tqdm(subset, desc=f\"Processing {section_key}\"):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=self.AZURE_DEPLOYMENT,\n",
    "                        messages=[{\"role\": \"user\", \"content\": self._build_prompt(text)}],\n",
    "                        temperature=1,\n",
    "                    )\n",
    "                    proposition_text = response.choices[0].message.content.strip()\n",
    "                    parsed = self._safe_parse_json(proposition_text)\n",
    "\n",
    "                    if parsed and isinstance(parsed, list):\n",
    "                        entry[\"chunks\"] = parsed\n",
    "                    else:\n",
    "                        entry[\"chunks\"] = None\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n Exception on entry: {text[:60]}...\\nError: {e}\")\n",
    "                    entry[\"chunks\"] = None\n",
    "\n",
    "            updated_data[section_key] = entries\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(updated_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\n Saved updated JSON with propositions to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e1128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
